{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework4 Word2Vec模型\n",
    "[参考CSDN博客](https://blog.csdn.net/weixin_50891266/article/details/116750204?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165244525516782184643706%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=165244525516782184643706&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-116750204-null-null.142^v9^pc_search_result_control_group,157^v4^control&utm_term=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%A4%A7%E4%BD%9C%E4%B8%9A&spm=1018.2226.3001.4187 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # 警告过滤器\n",
    "data_path = \"E:\\jupyter notebook\\homework4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words: 1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\WANGMI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.811 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people_names: 1237\n",
      "kungfu_names: 389\n",
      "sect_names: 97\n"
     ]
    }
   ],
   "source": [
    "# 加载停用词表\n",
    "stop_words_file = open(data_path + \"stop_words.txt\", 'r')\n",
    "stop_words = list()\n",
    "for line in stop_words_file.readlines():\n",
    "    line = line.strip()   # 去掉每行末尾的换行符\n",
    "    stop_words.append(line)\n",
    "stop_words_file.close()\n",
    "print('stop_words:',len(stop_words))\n",
    "\n",
    "# 导入金庸小说人物\n",
    "people_names_file = open(data_path + \"金庸小说全人物.txt\", 'r')\n",
    "people_names = list()\n",
    "for line in people_names_file.readlines():\n",
    "    line = line.strip()   # 去掉每行末尾的换行符\n",
    "    jieba.add_word(line)\n",
    "    people_names.append(line)\n",
    "stop_words_file.close()\n",
    "print('people_names:',len(people_names))\n",
    "\n",
    "# 导入金庸小说武功\n",
    "kungfu_names_file = open(data_path + \"金庸小说全武功.txt\", 'r')\n",
    "kungfu_names = list()\n",
    "for line in kungfu_names_file.readlines():\n",
    "    line = line.strip()   # 去掉每行末尾的换行符\n",
    "    jieba.add_word(line)\n",
    "    kungfu_names.append(line)\n",
    "stop_words_file.close()\n",
    "print('kungfu_names:',len(kungfu_names))\n",
    "\n",
    "# 导入金庸小说门派\n",
    "sect_names_file = open(data_path + \"金庸小说全门派.txt\", 'r')\n",
    "sect_names = list()\n",
    "for line in sect_names_file.readlines():\n",
    "    line = line.strip()   # 去掉每行末尾的换行符\n",
    "    jieba.add_word(line)\n",
    "    sect_names.append(line)\n",
    "stop_words_file.close()\n",
    "print('sect_names:',len(sect_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 书剑恩仇录.txt...\n",
      "书剑恩仇录.txt finished，with 3561 Row\n",
      "----------------------------------------\n",
      "Waiting for 侠客行.txt...\n",
      "侠客行.txt finished，with 3514 Row\n",
      "----------------------------------------\n",
      "Waiting for 倚天屠龙记.txt...\n",
      "倚天屠龙记.txt finished，with 7919 Row\n",
      "----------------------------------------\n",
      "Waiting for 天龙八部.txt...\n",
      "天龙八部.txt finished，with 10948 Row\n",
      "----------------------------------------\n",
      "Waiting for 射雕英雄传.txt...\n",
      "射雕英雄传.txt finished，with 7131 Row\n",
      "----------------------------------------\n",
      "Waiting for 白马啸西风.txt...\n",
      "白马啸西风.txt finished，with 597 Row\n",
      "----------------------------------------\n",
      "Waiting for 碧血剑.txt...\n",
      "碧血剑.txt finished，with 3786 Row\n",
      "----------------------------------------\n",
      "Waiting for 神雕侠侣.txt...\n",
      "神雕侠侣.txt finished，with 6999 Row\n",
      "----------------------------------------\n",
      "Waiting for 笑傲江湖.txt...\n",
      "笑傲江湖.txt finished，with 8551 Row\n",
      "----------------------------------------\n",
      "Waiting for 越女剑.txt...\n",
      "越女剑.txt finished，with 197 Row\n",
      "----------------------------------------\n",
      "Waiting for 连城诀.txt...\n",
      "连城诀.txt finished，with 2207 Row\n",
      "----------------------------------------\n",
      "Waiting for 雪山飞狐.txt...\n",
      "雪山飞狐.txt finished，with 1097 Row\n",
      "----------------------------------------\n",
      "Waiting for 飞狐外传.txt...\n",
      "飞狐外传.txt finished，with 3777 Row\n",
      "----------------------------------------\n",
      "Waiting for 鸳鸯刀.txt...\n",
      "鸳鸯刀.txt finished，with 213 Row\n",
      "----------------------------------------\n",
      "Waiting for 鹿鼎记.txt...\n",
      "鹿鼎记.txt finished，with 11159 Row\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "All finished，with 71656 Row\n"
     ]
    }
   ],
   "source": [
    "novel_path = data_path + '金庸小说库/'\n",
    "novel_names = list(os.listdir(novel_path))\n",
    "seg_novel = []\n",
    "for novel_name in novel_names:\n",
    "    novel = open(novel_path + novel_name, 'r', encoding='utf-8-sig')\n",
    "    print(\"Waiting for {}...\".format(novel_name))\n",
    "    line = novel.readline()\n",
    "    forward_rows = len(seg_novel)\n",
    "    while line:\n",
    "        line_1 = line.strip()\n",
    "        outstr = ''\n",
    "        line_seg = jieba.cut(line_1, cut_all=False)\n",
    "        for word in line_seg:  \n",
    "            if word not in stop_words:\n",
    "                if word != '\\t':\n",
    "                    if word[:2] in people_names:\n",
    "                        word = word[:2]\n",
    "                    outstr += word \n",
    "                    outstr += \" \"\n",
    "        if len(str(outstr.strip())) != 0:\n",
    "            seg_novel.append(str(outstr.strip()).split())\n",
    "        line = novel.readline()\n",
    "    print(\"{} finished，with {} Row\".format(novel_name, (len(seg_novel) - forward_rows)))\n",
    "    print(\"-\" * 40)\n",
    "print(\"-\" * 40)\n",
    "print(\"All finished，with {} Row\".format(len(seg_novel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models as w2v\n",
    "'''\n",
    "sentences为输入的语料\n",
    "vector_size为训练得到词向量的维数\n",
    "window为滑窗的大小\n",
    "min_count指过滤低频词\n",
    "sg为模型选择，0代表选择CBOW模型\n",
    "'''\n",
    "model = w2v.Word2Vec(sentences=seg_novel, vector_size=200, window=5, min_count=5, workers=4, sg=0)\n",
    "model.save(data_path + 'CBOW.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('康熙', 0.7160612344741821), ('公主', 0.6809028387069702), ('乾隆', 0.6571825742721558), ('祖千秋', 0.6326922178268433), ('小桂子', 0.6283841133117676), ('太后', 0.6254352927207947), ('苏菲亚', 0.6176230907440186), ('多隆', 0.6121008992195129), ('袁承志', 0.5981948375701904), ('茅十八', 0.5971606373786926)]\n",
      "[('令狐冲', 0.8151591420173645), ('虚竹', 0.794066309928894), ('张翠山', 0.7648916244506836), ('赵敏', 0.7640156149864197), ('苗人凤', 0.7619877457618713), ('黑白子', 0.7601329684257507), ('胡斐', 0.7582098245620728), ('袁紫衣', 0.750132143497467), ('范遥', 0.7400240898132324), ('杨过', 0.7344016432762146)]\n"
     ]
    }
   ],
   "source": [
    "model = w2v.Word2Vec.load(data_path + 'CBOW.model')\n",
    "print(model.wv.most_similar(\"韦小宝\", topn=10))\n",
    "print(model.wv.most_similar(\"张无忌\", topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7206061\n",
      "0.7640157\n",
      "0.45070273\n",
      "0.5289912\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('张无忌', '周芷若'))\n",
    "print(model.wv.similarity('张无忌', '赵敏'))\n",
    "print(model.wv.similarity('韦小宝', '阿珂'))\n",
    "print(model.wv.similarity('韦小宝', '双儿'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
